Operating Systems Lab
Assignment 4 Design Doc
By Group 4
Utsav Basu (20CS30057)
Anamitra Mukhopadhyay (20CS30064)
Rounak Saha (20CS30043)
Swarup Padhi (20CS30062)

Graph:
	The average number of neighbours of each `Node` in graph given = 289003 / 37700 ~= 8
	It isn't efficient to store an adjacency matrix for it. So the graph in stored as an adjacency list.
	`vector::shrink_to_fit()` has been called after loading the graph to get rid of extra unused memory.

Node:
	Since the average degree is not high, it's not efficient to store priority for all other `Node`s. So for when `feedQueue` is to be sorted by no. of common `Node`s, we simply calculated the respective priority at runtime.
	`vector<Nodes>` is the list of all `Node`s. It is used to quickly reference any `Node`, without accessing the graph everytime.
	Each `Node` stores a `wallQueue`, a `feedQueue`, a counter of `past_action`s, a pointer to its list of `neighbours`, and `mutex` for `feedQueue`.

Action:
	Nothing special. Simply stores `action_id`, `user_id` of the user who generated the action, `type` of action, `timestamp` of action generation

Queue:
	A special multi-queue data structure created by us to streamline the efficiency of the threads.

	Contains N `std::queue`s, a `mutex` and a `conditional_variable` for each, a counter, a `mutex` for pushing to the queue.

	An N-way queue provides the usual queue functionalities of push and pop but is internally implemented as N independent standard FIFO queues.
	The `Queue` maintains a counter referring to the index of the underlying queue to which an element is to be added when the next push call is made, the push function does not provide the control of choosing the index explicitly to the user. The counter is incremented in a round robin fashion, which means the immediate numeric successor (mod N) is selected for insertion for the next push call. 
	The pop function gives control over the index of the internal queue to pop from and takes the same as a function parameter.
	The locking mechanism to maintain synchronisation across multiple threads/processes accessing the multiway queue is handled internally inside the push and pop functions, a user thread/process need not bother about locking/unlocking while accessing the queue. The counter is guarded by push_mutex and each of the internal queues is guarded by a different mutex lock, the only contenders for access being reader(pop) and writer(push) processes, there is no contention for accessing queues with different indexes.

sns.cpp (Main Thread):
	1. Reads and loads the graph to memory. Every adjacency list is sorted since `std::set_intersection()` needs to be called later for assigning priority in `feedQueue` of `Node`s.
	2. Spawns 36 threads (1 for `userSimulator`, 25 for `pushUpdate`, 10 for `readPost`).

userSimulator.cpp (User Simulator Thread):
	Wakes up every 120s and generates random `Action`s proportional to the `Node`'s degree for 100 random `Node`s, pushing every of them to `shared_queue`.

pushUpdate.cpp (Push Update Thread):
	Pops from `shared_queue` the next `Action` to handle, and 
		1. Updates `feedQueue` of all `Node`s who are the neighbours of the `Node` which generated the action.
		2. Pushes the affected `Node`s to `updates`. (Note a `Node` maybe pushed into `updates` multiple times. This is not an issue as explained later.)

readPost.cpp (Read Post Thread):
	Pops from `updates` next `Node` to service, and reads its full `feedQueue`.

Justification of locks, and other Data Structures:
	Main thread creates two special `Queue`s, `shared_queue` and `updates`.
	In those cases, where a single queue is being serviced by multiple threads, little parallelism is achieved if there is only one point of entry/exit from the queue. To streamline the access, we make multiple access points in the queue which are independently served by different threads, there is no access contention between different queues and hence the threads don't block each other for access. Thus it makes sense to have as many access points (internal queues) as there are threads (fewer access points would lead to redundant threads and more access points would lead to redundancy of access points).
	The `shared_queue` (into which userSimulator enters actions and `pushUpdate` reads from) is implemented as a 25-way queue as there are 25 `pushUpdate` threads reading from it. This also facilitates user convenience in the following sense:
	For a normal shared queue, consider the case of very popular users who, according to our design, perform more actions. When one such user A performs a bulk of actions, they get lined up to be serviced from the service queue. From the point the first action of this thread is serviced by a `pushUpdate` thread, all the other `pushUpdate` threads would service keep serving actions from user A only untill all its actions are not popped (note that there is a single userSimulator thread and hence actions of user A appear in a row in the queue without actions from anyone else appearing between them). This problem is solved on using a multiway queue.
	A similar problem arises for `readPost`. We need to tell the `readPost` threads which `Node`s' `feedQueue` to read from the `pushUpdate` threads. Again, a 10-way queue helps in this case. We insert the updated `Node`s' indexes to the `updates` queue in a round-robin manner from `pushUpdate` threads. `readPost` threads now pop from their respective queue and service each `Node`. It might arise sometimes that the same `Node` is pushed multiple times. However, even in that case, since we are checking if the `Node`'s `feedQueue` is empty or not, there's no problem. If same `Node` is encountered again then it is simply ignored, if its `feedQueue` empty.
	In each `Node` we have a `mutex` to guard the `feedQueue`. It is needed to ensure mutual exclusion in events of push (in pushUpdate threads) and pop (in readPost threads).
	In `Queue` we have a `push_mutex` for counter, one `mutex` and one `conditional_variable` each for N queues. `push_mutex` ensures mutual exclusion of counter, whenever a new element is to be pushed to any queue. `mutexes[i]` ensures mutual exclusion for events push/pop in `queues[i]`. `conditionals[i]` is used to signal/wait on `queues[i]`.


Some Estimates:
	We did some analytics on the Graph and found out these results:
		Max size of `feedQueue` ~= 11k
		Avg size of `feedQueue` ~= 4.5k
		Max size of `shared_queue` ~= 11k
	
